{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install dlt[duckdb] alive-progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "dlt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sources\n",
    "dlt.source\n",
    "def dezoomcamp_source():\n",
    "    client = RESTClient(\n",
    "        base_url='https://us-central1-dlthub-analytics.cloudfunctions.net',\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page = 1,\n",
    "            total_path= None\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    dlt.resource(table_name='rides')\n",
    "    def rides():\n",
    "        for page in client.paginate('data_engineering_zoomcamp_api'):\n",
    "            yield page\n",
    "            \n",
    "    return rides\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline for extraction, normalizing, and loading\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name = 'ny_taxi_pipeline',\n",
    "    destination = 'duckdb',\n",
    "    dataset_name= 'ny_taxi_data',\n",
    "    progress= 'alive_progress'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 14:58:30,858|[WARNING]|3740965|125654417277056|dlt|configuration.py|_path_from_pipeline:178|Duckdb attached to pipeline ny_taxi_pipeline in path ny_taxi_pipeline.duckdb was could not be found but pipeline has already ran. This may be a result of (1) recreating or attaching pipeline  without or with changed explicit path to database that was used when creating the pipeline. (2) keeping the path to to database in secrets and changing the current working folder so  dlt cannot see them. (3) you deleting the database.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract ny_taxi: Resources |████████████████████████████████████████| 1/1 [100%] in 20.4s (0.05/s) \n",
      "Normalize ny_taxi in 1739649510.898063: Files |████████████████████████████████████████| 1/1 [100%] in 0.5s (1.97/s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 14:58:51,832|[WARNING]|3740965|125654417277056|dlt|configuration.py|_path_from_pipeline:178|Duckdb attached to pipeline ny_taxi_pipeline in path ny_taxi_pipeline.duckdb was could not be found but pipeline has already ran. This may be a result of (1) recreating or attaching pipeline  without or with changed explicit path to database that was used when creating the pipeline. (2) keeping the path to to database in secrets and changing the current working folder so  dlt cannot see them. (3) you deleting the database.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ny_taxi in 1739649510.898063: Jobs |████████████████████████████████████████| 1/1 [100%] in 0.8s (1.24/s) \n",
      "Pipeline ny_taxi_pipeline load step completed in 0.81 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset ny_taxi_data\n",
      "The duckdb destination used duckdb:////media/isaac/DATA/Documents/dezoomcamp/dlt_workshop_homework/ny_taxi_pipeline.duckdb location to store data\n",
      "Load package 1739649510.898063 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "load_info = pipeline.run(dezoomcamp_source(), write_disposition=\"replace\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>schema</th>\n",
       "      <th>name</th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_types</th>\n",
       "      <th>temporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ny_taxi_pipeline</td>\n",
       "      <td>ny_taxi_data</td>\n",
       "      <td>_dlt_loads</td>\n",
       "      <td>[load_id, schema_name, status, inserted_at, sc...</td>\n",
       "      <td>[VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ny_taxi_pipeline</td>\n",
       "      <td>ny_taxi_data</td>\n",
       "      <td>_dlt_pipeline_state</td>\n",
       "      <td>[version, engine_version, pipeline_name, state...</td>\n",
       "      <td>[BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP W...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ny_taxi_pipeline</td>\n",
       "      <td>ny_taxi_data</td>\n",
       "      <td>_dlt_version</td>\n",
       "      <td>[version, engine_version, inserted_at, schema_...</td>\n",
       "      <td>[BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VAR...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ny_taxi_pipeline</td>\n",
       "      <td>ny_taxi_data</td>\n",
       "      <td>rides</td>\n",
       "      <td>[end_lat, end_lon, fare_amt, passenger_count, ...</td>\n",
       "      <td>[DOUBLE, DOUBLE, DOUBLE, BIGINT, VARCHAR, DOUB...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           database        schema                 name  \\\n",
       "0  ny_taxi_pipeline  ny_taxi_data           _dlt_loads   \n",
       "1  ny_taxi_pipeline  ny_taxi_data  _dlt_pipeline_state   \n",
       "2  ny_taxi_pipeline  ny_taxi_data         _dlt_version   \n",
       "3  ny_taxi_pipeline  ny_taxi_data                rides   \n",
       "\n",
       "                                        column_names  \\\n",
       "0  [load_id, schema_name, status, inserted_at, sc...   \n",
       "1  [version, engine_version, pipeline_name, state...   \n",
       "2  [version, engine_version, inserted_at, schema_...   \n",
       "3  [end_lat, end_lon, fare_amt, passenger_count, ...   \n",
       "\n",
       "                                        column_types  temporary  \n",
       "0  [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME...      False  \n",
       "1  [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP W...      False  \n",
       "2  [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VAR...      False  \n",
       "3  [DOUBLE, DOUBLE, DOUBLE, BIGINT, VARCHAR, DOUB...      False  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question 2\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(f'{pipeline.pipeline_name}.duckdb')\n",
    "\n",
    "con.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "\n",
    "con.sql('DESCRIBE').df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────┐\n",
       "│ count_star() │\n",
       "│    int64     │\n",
       "├──────────────┤\n",
       "│        10000 │\n",
       "└──────────────┘"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3\n",
    "con.sql('select count(*) from rides')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 15:03:26,619|[WARNING]|3740965|125654417277056|dlt|configuration.py|_path_from_pipeline:178|Duckdb attached to pipeline ny_taxi_pipeline in path ny_taxi_pipeline.duckdb was could not be found but pipeline has already ran. This may be a result of (1) recreating or attaching pipeline  without or with changed explicit path to database that was used when creating the pipeline. (2) keeping the path to to database in secrets and changing the current working folder so  dlt cannot see them. (3) you deleting the database.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12.3049,)]\n"
     ]
    }
   ],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    res = client.execute_sql(\n",
    "            \"\"\"\n",
    "            SELECT\n",
    "            AVG(date_diff('minute', trip_pickup_date_time, trip_dropoff_date_time))\n",
    "            FROM rides;\n",
    "            \"\"\"\n",
    "        )\n",
    "    # Prints column values of the first row\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
